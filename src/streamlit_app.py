# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  Crossâ€‘Check Framework  â€¢  StreamlitÂ Simulation App
#  v2025â€‘07â€‘29  â€“  SobolÂ global sensitivity & PCG64DXSM RNG
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  (c) 2025 digitalian  â€“  MITÂ License
# ----------------------------------------------------------------
#  Changelog
#   â€¢ NEW  : Sobol firstâ€‘order/global sensitivity chart (SALib)
#   â€¢ NEW  : PCG64DXSM BitGenerator for faster, robust RNG
#   â€¢ FIX  : BUGâ€‘1, BUGâ€‘4  (see previous commits)
# ----------------------------------------------------------------

from __future__ import annotations

import streamlit as st
import numpy as np, math
import sympy as sp
import pandas as pd
import plotly.express as px
from typing import Tuple, Dict, List

# â”€â”€ New: SALib for Sobol â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
try:
    # SALib â‰¥1.5 æ¨å¥¨ãƒ«ãƒ¼ãƒˆ
    from SALib.sample.sobol import sample as sobol_sample
except ImportError:          # SALib â‰¤1.4 fallback
    from SALib.sample import saltelli as sobol_sample
from SALib.analyze import sobol

# â”€â”€ New: PCG64DXSM generator (faster & parallelâ€‘safe) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
from numpy.random import PCG64DXSM, Generator

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
#  SectionÂ 1  â€¢  Helper utilities
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def get_text_labels(lang: str) -> Dict:
    """Return languageâ€‘specific label dictionary."""
    LANG = "EN" if lang == "English" else "JA"
    return TXT_ALL[LANG]


def get_state(key, default):
    """Retrieve or initialise a value in `st.session_state`."""
    if key not in st.session_state:
        st.session_state[key] = default
    return st.session_state[key]


def exp(path: str):
    """
    Create an expander from nested TXT dict using a dotâ€‘separated key path.
    Example: 'charts.tornado.expander_title'
    """
    keys = path.split(".")
    title = TXT
    for k in keys:
        title = title[k]
    content_keys = keys[:-1] + [keys[-1].replace("_title", "_content")]
    content = TXT
    for k in content_keys:
        content = content[k]
    with st.expander(title, expanded=False):
        st.markdown(content)

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
#  SectionÂ 2  â€¢  Core deterministic model
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def compute_metrics(
    a1v: float,
    a2v: float,
    a3v: float,
    bv: float,
    cross_ratio_v: float,
    prep_post_ratio_v: float,
    loss_unit_v: float,
    qualv: str,
    schedv: str,
    t1v: float,
    t2v: float,
    t3v: float,
) -> Tuple[float, float, float, float, float]:
    """Return S, C, C_loss, E, E_total for a single scenario."""
    # Multipliers
    qual_T, qual_B = (1, 1) if qualv == "Standard" else (2 / 3, 0.8)
    sched_T, sched_B = (1, 1) if schedv == "OnTime" else (2 / 3, 0.8)

    # Success probability
    a_tot = a1v * a2v * a3v
    b_eff = bv * qual_B * sched_B
    S_x = 1 - (1 - a_tot) * (1 - b_eff)

    # Costs
    T = (t1v + t2v + t3v) * qual_T * sched_T
    C_x = T * (1 + cross_ratio_v + prep_post_ratio_v)
    C_loss_x = C_x + loss_unit_v * C_x * (1 - S_x)

    # Efficiencies
    E_x = C_x / S_x
    E_total_x = C_loss_x / S_x
    return S_x, C_x, C_loss_x, E_x, E_total_x


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
#  SectionÂ 3  â€¢  Sensitivityâ€‘plot helper
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def make_sensitivity_bar(
    df: pd.DataFrame, value_col: str, tick_fmt: str = "{:.2f}", order=None
):
    """Horizontal bar (Plotly) for sensitivity tables."""
    df = df.copy()
    df[value_col] = df[value_col].abs()
    if order:
        df["Parameter"] = pd.Categorical(df["Parameter"], categories=order, ordered=True)
    fig = px.bar(
        df,
        x=value_col,
        y="Parameter",
        orientation="h",
        text=df[value_col].map(tick_fmt.format),
        color_discrete_sequence=["#000000"],
    )
    fig.update_traces(
        texttemplate="%{text}",
        insidetextfont_color="white",
        outsidetextfont_color="gray",
    )
    fig.update_layout(
        showlegend=False,
        yaxis=dict(categoryorder="array", categoryarray=order) if order else {},
        font=dict(size=14),
        bargap=0.1,
        margin=dict(t=30, b=40),
    )
    return fig


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
#  SectionÂ 4  â€¢  Streamlitâ€‘wide config & localisation strings
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
st.set_page_config(page_title="Crossâ€‘Check Simulator", layout="wide")

if "lang" not in st.session_state:
    st.session_state.lang = "English"
lang = st.sidebar.radio("Language / è¨€èª", ["English", "æ—¥æœ¬èª"], index=0, key="lang", horizontal=True)

# --- Localisation dictionaries (EN & JA) ---
#   *Sobol éƒ¨åˆ†ã®æ–°ãƒ©ãƒ™ãƒ«ã‚’è¿½è¨˜*
TXT_EN = {
    "panel": {
        "input": "INPUT PANEL",
        "output": "KEY OUTPUT METRICS",
    },
    "metrics": {
        "a_total": "a_total",
        "succ": "Success Rate S",
        "C": "Labor Cost C",
        "Closs": "C_total (with loss)",
        "loss_unit": "Loss unit â„“",
        "E_base": "Efficiency E (baseline)",
        "E_total": "E_total",
    },
    "charts": {
        "quality_schedule": {
            "title": "Quality Ã— Schedule 2Ã—2 Matrix",
            "expander_title": "ğŸ“˜ About Quality Ã— Schedule Chart",
            "expander_content": (
                "Each bar shows E_total under different combinations of quality and schedule, "
                "with labels showing the corresponding success rate.  \n\n"
                "This helps compare cost-performance tradeoffs across operational scenarios."
            ),
        },
        "tornado": {
            "title": "Tornado Sensitivity",
            "expander_title": "ğŸ“˜ Explanation: Impact of Â±20% Parameter Changes",
            "expander_content": (
                "This chart visualizes the effect of Â±20% changes in key parameters on E_total (cost per success).  \n"
                "Selected parameters (aâ‚, aâ‚‚, aâ‚ƒ, bâ‚€, CR, PP, â„“) are core drivers of success, effort, and loss.  \n"
                "This helps identify which inputs most strongly affect cost-efficiency."
            ),
            "xaxis": "|Î”E/E|",
        },
        "sobol": {
            "title": "Global Sensitivity (Sobol Sâ‚)",
            "expander_title": "ğŸ“˜ Sobol Global Sensitivity",
            "expander_content": (
                "Varianceâ€‘based global sensitivity analysis using Saltelli sampling (NÃ—(k+2) runs). "
                "Bars show the firstâ€‘order Sobol index Sâ‚ for each parameter; "
                "higher values = greater contribution to output variance."
            ),
            "xaxis": "Sobol Sâ‚",
        },
        "relative_sensitivity": {
            "title": "Relative Sensitivity",
            "xaxis": "Relative Sensitivity (âˆ‚E/âˆ‚x Ã— x/E)",
            "expander_title": "ğŸ“˜ About Relative Sensitivity",
            "expander_content": """This chart displays the relative sensitivity of E_total (loss-adjusted cost per success) with respect to three independent parameters: loss unit â„“, labor cost C, and success rate S.  
It quantifies the elasticity (âˆ‚E/âˆ‚x Ã— x/E) for each parameter, showing how a 1% proportional change impacts overall cost efficiency.  
Use this analysis to prioritize which factor most improves cost efficiency when adjusted.""",
        },
        "standardized_sensitivity": {
            "title": "Standardized Sensitivity ",
            "xaxis": "Standardized Sensitivity (Î”E/Ïƒ_E)",
            "expander_title": "ğŸ“˜ About Standardized Sensitivity",
            "expander_content": """This chart shows the standardized sensitivity of E_total with respect to loss unit â„“, labor cost C, and success rate S.  
It normalizes each parameterâ€™s partial derivative by its variability (âˆ‚E/âˆ‚x Ã— Ïƒâ‚“/Ïƒ_E) to reveal which uncertainties contribute most to efficiency variance.  
Use this analysis for risk assessment and uncertainty management.""",
        },
        "monte_carlo": {
            "title": "Monte Carlo Summary Statistics",
            "variable": "MC variable",
            "expander_title": "ğŸ“˜ Explanation: Monte Carlo Parameter Distributions",
            "expander_content": (
                "The following parameters are assigned probabilistic distributions to capture plausible uncertainty ranges:  \n\n"
                "- **aâ‚, aâ‚‚**: Normally distributed (mean = selected value, Ïƒ = 0.03), reflecting variation in basic process success rates due to human or environmental variability.  \n"
                "- **aâ‚ƒ**: Triangular distribution (Â±10%) to reflect process-specific asymmetry in the final step's reliability.  \n"
                "- **bâ‚€**: Uniform between 0.70â€“0.90, assuming checker quality varies widely across contexts.  \n"
                "- **Cross-ratio (CR), Prep/Post ratio (PP)**: Triangular (Â±20%) around selected values to reflect managerial estimation variance.  \n"
                "- **Loss unit â„“**: Triangular (Â±20%) for capturing business risk variability.  \n\n"
                "These distributions are selected based on empirical heuristics: normal for stable processes (aâ‚, aâ‚‚), triangular for bounded uncertain estimates (aâ‚ƒ, CR, PP, â„“), and uniform for quality variability (bâ‚€)."
            ),
            "mean": "Mean",
            "median": "Median",
            "ci": "5â€“95% CI",
            "caption": {
                "en": "Monte Carlo simulation output distributions with median, mean (solid), and 5â€“95% CI (dotted)",
                "ja": "ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­æ³•ã«ã‚ˆã‚‹å‡ºåŠ›åˆ†å¸ƒï¼šä¸­å¤®å€¤ãƒ»å¹³å‡å€¤ï¼ˆå®Ÿç·šï¼‰ã€ä¿¡é ¼åŒºé–“5â€“95%ï¼ˆç‚¹ç·šï¼‰"
            },
            "card_unit": "[E_total]",
        }
    }
}
TXT_JA = {
    "panel": {
        "input": "å…¥åŠ›ãƒ‘ãƒãƒ«",
        "output": "ä¸»è¦å‡ºåŠ›æŒ‡æ¨™",
    },
    "metrics": {
        "a_total": "a_total",
        "succ": "æˆåŠŸç‡ S",
        "C": "Cï¼ˆä½œæ¥­å·¥æ•°ï¼‰",
        "Closs": "C_totalï¼ˆæå¤±è¾¼ï¼‰",
        "loss_unit": "æå¤±å˜ä¾¡ â„“",
        "E_base": "åŠ¹ç‡ Eï¼ˆãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼‰",
        "E_total": "E_total",
    },
    "charts": {
        "quality_schedule": {
            "title": "å“è³ªÃ—ç´æœŸã®2Ã—2ãƒãƒˆãƒªã‚¯ã‚¹",
            "expander_title": "ğŸ“˜ å“è³ª Ã— ç´æœŸã‚°ãƒ©ãƒ•ã«ã¤ã„ã¦",
            "expander_content": (
                "å„ãƒãƒ¼ã¯å“è³ªãƒ»ç´æœŸã®çµ„ã¿åˆã‚ã›ã”ã¨ã®E_totalï¼ˆæˆåŠŸ1ä»¶ã‚ãŸã‚Šç·ã‚³ã‚¹ãƒˆï¼‰ã‚’ç¤ºã—ã€"
                "ãƒ©ãƒ™ãƒ«ã¯ãã®æ™‚ã®æˆåŠŸç‡ã§ã™ã€‚  \n\n"
                "é‹ç”¨ã‚·ãƒŠãƒªã‚ªã”ã¨ã®ã‚³ã‚¹ãƒˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®é•ã„ã‚’æ¯”è¼ƒã§ãã¾ã™ã€‚"
            ),
        },
        "tornado": {
            "title": "ãƒˆãƒ«ãƒãƒ¼ãƒ‰æ„Ÿåº¦åˆ†æ",
            "expander_title": "ğŸ“˜ èª¬æ˜ï¼šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿Â±20ï¼…å¤‰åŒ–ã®å½±éŸ¿",
            "expander_content": (
                "ã“ã®ã‚°ãƒ©ãƒ•ã¯ä¸»è¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆaâ‚, aâ‚‚, aâ‚ƒ, bâ‚€, CR, PP, â„“ï¼‰ã‚’Â±20%å¤‰åŒ–ã•ã›ãŸã¨ãã®E_totalï¼ˆæˆåŠŸ1ä»¶ã‚ãŸã‚Šç·ã‚³ã‚¹ãƒˆï¼‰ã¸ã®å½±éŸ¿ã‚’ç¤ºã—ã¾ã™ã€‚  \n"
                "ã©ã®å…¥åŠ›ãŒã‚³ã‚¹ãƒˆåŠ¹ç‡ã«æœ€ã‚‚å¼·ãå½±éŸ¿ã™ã‚‹ã‹ã‚’å¯è¦–åŒ–ã—ã¾ã™ã€‚"
            ),
            "xaxis": "|Î”E/E|",
        },
        "sobol": {
            "title": "ã‚°ãƒ­ãƒ¼ãƒãƒ«æ„Ÿåº¦ (Sobol Sâ‚)",
            "expander_title": "ğŸ“˜ Sobol ã‚°ãƒ­ãƒ¼ãƒãƒ«æ„Ÿåº¦",
            "expander_content": (
                "Saltelli ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã«ã‚ˆã‚‹ Sobol ä¸€æ¬¡æŒ‡æ•° (Sâ‚)ã€‚"
                "E_total ã®åˆ†æ•£ã«å¯¾ã™ã‚‹å„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å¯„ä¸åº¦ã‚’ç¤ºã—ã¾ã™ã€‚"
            ),
            "xaxis": "Sobol Sâ‚",
        },
        "relative_sensitivity": {
            "title": "ç›¸å¯¾æ„Ÿåº¦",
            "xaxis": "ç›¸å¯¾æ„Ÿåº¦ï¼ˆâˆ‚E/âˆ‚x Ã— x/Eï¼‰",
            "expander_title": "ğŸ“˜ ç›¸å¯¾æ„Ÿåº¦ã‚°ãƒ©ãƒ•ã«ã¤ã„ã¦",
            "expander_content": """äº’ã„ã«ç‹¬ç«‹ã—ãŸå„æŒ‡æ¨™ï¼ˆæå¤±å˜ä¾¡â„“ã€Cï¼ˆä½œæ¥­å·¥æ•°ï¼‰ã€æˆåŠŸç‡Sï¼‰ãŒ1%å¤‰åŒ–ã—ãŸã¨ãã®E_totalï¼ˆç·åˆã‚³ã‚¹ãƒˆåŠ¹ç‡ï¼‰ã®å¤‰åŒ–ç‡ï¼ˆå¼¾æ€§å€¤ï¼‰ã‚’ç¤ºã—ã¾ã™ã€‚  
è¨­è¨ˆã‚„é‹ç”¨æ”¹å–„ã®å„ªå…ˆåº¦ã‚’è€ƒãˆã‚‹ä¸Šã§ã€ã©ã®å› å­ãŒåŠ¹ç‡ã«æœ€ã‚‚å½±éŸ¿ã™ã‚‹ã‹æŠŠæ¡ã§ãã¾ã™ã€‚""",
        },
        "standardized_sensitivity": {
            "title": "æ¨™æº–åŒ–æ„Ÿåº¦",
            "xaxis": "æ¨™æº–åŒ–æ„Ÿåº¦ï¼ˆÎ”E/Ïƒ_Eï¼‰",
            "expander_title": "ğŸ“˜ æ¨™æº–åŒ–æ„Ÿåº¦ã‚°ãƒ©ãƒ•ã«ã¤ã„ã¦",
            "expander_content": """äº’ã„ã«ç‹¬ç«‹ã—ãŸå„æŒ‡æ¨™ï¼ˆæå¤±å˜ä¾¡â„“ã€Cï¼ˆä½œæ¥­å·¥æ•°ï¼‰ã€æˆåŠŸç‡Sï¼‰ã®ã°ã‚‰ã¤ãï¼ˆæ¨™æº–åå·®ï¼‰ã§æ­£è¦åŒ–ã—ãŸE_totalã¸ã®å½±éŸ¿åº¦ã‚’ç¤ºã—ã¾ã™ã€‚  
ä¸ç¢ºå®Ÿæ€§ã«ã‚ˆã‚‹ãƒªã‚¹ã‚¯è©•ä¾¡ã‚„ã€ã©ã®å› å­ã®åˆ†æ•£ãŒã‚³ã‚¹ãƒˆåŠ¹ç‡ã®ä¸å®‰å®šã•ã«å¯„ä¸ã—ã¦ã„ã‚‹ã‹ã‚’æŠŠæ¡ã§ãã¾ã™ã€‚""",
        },
        "monte_carlo": {
            "title": "ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­è¦ç´„çµ±è¨ˆ",
            "variable": "MCå¯¾è±¡å¤‰æ•°",
            "expander_title": "ğŸ“˜ èª¬æ˜ï¼šãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­ã«ãŠã‘ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åˆ†å¸ƒ",
            "expander_content": (
                "ä»¥ä¸‹ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«ä¸ç¢ºå®Ÿæ€§ï¼ˆåˆ†å¸ƒï¼‰ã‚’ä»®å®šã—ã¦ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’è¡Œã„ã¾ã™ï¼š  \n\n"
                "- **aâ‚, aâ‚‚**ï¼šæ­£è¦åˆ†å¸ƒï¼ˆå¹³å‡=é¸æŠå€¤ã€Ïƒ=0.03ï¼‰ã§ã€äººã‚„ç’°å¢ƒã«ã‚ˆã‚‹ã°ã‚‰ã¤ãã‚’åæ˜   \n"
                "- **aâ‚ƒ**ï¼šä¸‰è§’åˆ†å¸ƒï¼ˆÂ±10%ï¼‰ã§æœ€çµ‚å·¥ç¨‹ã®éå¯¾ç§°ãªä¿¡é ¼æ€§ã‚’è¡¨ç¾  \n"
                "- **bâ‚€**ï¼šä¸€æ§˜åˆ†å¸ƒï¼ˆ0.70â€“0.90ï¼‰ã§ãƒã‚§ãƒƒã‚«ãƒ¼å“è³ªã®å¹…åºƒã„çŠ¶æ³ã‚’æƒ³å®š  \n"
                "- **ã‚¯ãƒ­ã‚¹æ¯”ï¼ˆCRï¼‰ã€æº–å‚™ãƒ»å¾Œå‡¦ç†æ¯”ï¼ˆPPï¼‰**ï¼šä¸‰è§’åˆ†å¸ƒï¼ˆÂ±20%ï¼‰ã§è¦‹ç©ã‚Šèª¤å·®ã‚’åæ˜   \n"
                "- **æå¤±å˜ä¾¡ â„“**ï¼šä¸‰è§’åˆ†å¸ƒï¼ˆÂ±20%ï¼‰ã§ãƒ“ã‚¸ãƒã‚¹ãƒªã‚¹ã‚¯ã®å¹…ã‚’è¡¨ç¾  \n\n"
                "åˆ†å¸ƒã®é¸æŠã¯çµŒé¨“å‰‡ã«åŸºã¥ãã€å®‰å®šå·¥ç¨‹ï¼ˆaâ‚, aâ‚‚ï¼‰ã¯æ­£è¦ã€æ¨å®šå€¤ï¼ˆaâ‚ƒ, CR, PP, â„“ï¼‰ã¯ä¸‰è§’ã€ä¸€æ§˜ï¼ˆbâ‚€ï¼‰ã¯å“è³ªã®å¹…åºƒã•ã‚’æƒ³å®šã—ã¦ã„ã¾ã™ã€‚"
            ),
            "mean": "å¹³å‡å€¤",
            "median": "ä¸­å¤®å€¤",
            "ci": "ä¿¡é ¼åŒºé–“5â€“95%",
            "caption": {
                "en": "Monte Carlo simulation output distributions with median, mean (solid), and 5â€“95% CI (dotted)",
                "ja": "ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­æ³•ã«ã‚ˆã‚‹å‡ºåŠ›åˆ†å¸ƒï¼šä¸­å¤®å€¤ãƒ»å¹³å‡å€¤ï¼ˆå®Ÿç·šï¼‰ã€ä¿¡é ¼åŒºé–“5â€“95%ï¼ˆç‚¹ç·šï¼‰"
            },
            "card_unit": "[E_total]",
        }
    }
}
TXT_ALL = {"EN": TXT_EN, "JA": TXT_JA}
TXT = get_text_labels(lang)

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
#  SectionÂ 5  â€¢  Sidebar inputs
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def get_sidebar_params() -> Dict[str, float]:
    st.sidebar.title(TXT["panel"]["input"])
    # 1) Success rates
    a1 = st.sidebar.slider("a1 (stepâ€¯1)", 0.5, 1.0, get_state("a1", 0.95), 0.01)
    a2 = st.sidebar.slider("a2 (stepâ€¯2)", 0.5, 1.0, get_state("a2", 0.95), 0.01)
    a3 = st.sidebar.slider("a3 (stepâ€¯3)", 0.5, 1.0, get_state("a3", 0.80), 0.01)
    # 2) Task times
    col_t1, col_t2, col_t3 = st.sidebar.columns(3)
    with col_t1:
        T1 = st.number_input("T1â€¯[h]", 0, 200, get_state("T1", 10), key="T1")
    with col_t2:
        T2 = st.number_input("T2â€¯[h]", 0, 200, get_state("T2", 10), key="T2")
    with col_t3:
        T3 = st.number_input("T3â€¯[h]", 0, 200, get_state("T3", 30), key="T3")
    # 3) Quality & Schedule
    col_q, col_s = st.sidebar.columns(2)
    with col_q:
        qual = st.selectbox("Quality", ["Standard", "Low"], index=0 if get_state("qual", "Standard") == "Standard" else 1, key="qual")
    with col_s:
        sched = st.selectbox("Schedule", ["OnTime", "Late"], index=0 if get_state("sched", "OnTime") == "OnTime" else 1, key="sched")
    # 4) Checker
    st.sidebar.markdown("---")
    b0 = st.sidebar.slider("b0â€¯(checker)", 0.0, 1.0, get_state("b0", 0.80), 0.01)
    # 5) Cost ratios
    cross_ratio = st.sidebar.slider("Crossâ€‘ratio", 0.0, 0.5, get_state("cross_ratio", 0.30), 0.01)
    prep_post_ratio = st.sidebar.slider("Prep/Post ratio", 0.0, 0.5, get_state("prep_post_ratio", 0.40), 0.01)
    # 6) Loss unit
    st.sidebar.markdown("---")
    loss_unit = st.sidebar.slider("Loss unit â„“", 0.0, 50.0, get_state("loss_unit", 0.0), 0.1)
    # 7) Monte Carlo
    st.sidebar.markdown("<hr style='border-top:3px solid black'>", unsafe_allow_html=True)
    st.sidebar.title("Monteâ€‘Carlo")
    col_n, col_var = st.sidebar.columns(2)
    with col_n:
        sample_n = st.number_input("Samples", 1_000, 1_000_000, get_state("sample_n", 100_000), step=10_000)
    with col_var:
        mc_var = st.selectbox("MC variable", ["E_total", "Success S"], 0, key="mc_var")

    return dict(
        a1=a1,
        a2=a2,
        a3=a3,
        b0=b0,
        qual=qual,
        sched=sched,
        loss_unit=loss_unit,
        T1=T1,
        T2=T2,
        T3=T3,
        cross_ratio=cross_ratio,
        prep_post_ratio=prep_post_ratio,
        sample_n=sample_n,
        mc_var=mc_var,
    )

params = get_sidebar_params()

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
#  SectionÂ 6  â€¢  Monteâ€‘Carlo simulation  (PCG64DXSM)
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
@st.cache_data(show_spinner=False, ttl=900)
def run_mc(p: Dict[str, float], N: int) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    """Vectorised Monteâ€‘Carlo returning Evals, Svals, Cvals, L_samples."""
    rng = Generator(PCG64DXSM(seed=0))

    # Success params
    a1s = rng.normal(p["a1"], 0.03, N).clip(0, 1)
    a2s = rng.normal(p["a2"], 0.03, N).clip(0, 1)
    a3s = rng.triangular(p["a3"] * 0.9, p["a3"], p["a3"] * 1.1, N).clip(0, 1)
    b0s = rng.uniform(0.70, 0.90, N)

    # Task times (Â±10â€¯%)
    t1s = rng.normal(p["T1"], p["T1"] * 0.1, N).clip(min=1)
    t2s = rng.normal(p["T2"], p["T2"] * 0.1, N).clip(min=1)
    t3s = rng.normal(p["T3"], p["T3"] * 0.1, N).clip(min=1)

    # Cost ratios
    cross_ratios = rng.triangular(p["cross_ratio"] * 0.8, p["cross_ratio"], p["cross_ratio"] * 1.2, N) if p["cross_ratio"] > 0 else np.zeros(N)
    prep_post_ratios = rng.triangular(p["prep_post_ratio"] * 0.8, p["prep_post_ratio"], p["prep_post_ratio"] * 1.2, N) if p["prep_post_ratio"] > 0 else np.zeros(N)
    L_samples = rng.triangular(p["loss_unit"] * 0.8, p["loss_unit"], p["loss_unit"] * 1.2, N) if p["loss_unit"] > 0 else np.zeros(N)

    # Multipliers
    qual_T, qual_B = (1, 1) if p["qual"] == "Standard" else (2 / 3, 0.8)
    sched_T, sched_B = (1, 1) if p["sched"] == "OnTime" else (2 / 3, 0.8)

    a_tot = a1s * a2s * a3s
    b_eff = b0s * qual_B * sched_B
    Svals = 1 - (1 - a_tot) * (1 - b_eff)

    Tvals = (t1s + t2s + t3s) * qual_T * sched_T
    Cvals = Tvals * (1 + cross_ratios + prep_post_ratios)
    Evals = (Cvals + L_samples * Cvals * (1 - Svals)) / Svals
    return Evals, Svals, Cvals, L_samples


Evals, Svals, Cvals, L_samples = run_mc(params, int(params["sample_n"]))

# Stdâ€‘devs
ÏƒE, ÏƒC, ÏƒS, ÏƒL = Evals.std(), Cvals.std(), Svals.std(), (Cvals * L_samples).std()

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
#  SectionÂ 7  â€¢  Sobol global sensitivity  (PCG64DXSM)
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
@st.cache_data(show_spinner=False, ttl=900)
def run_sobol(p: Dict[str, float], N: int = 10_000) -> pd.DataFrame:
    """Return DataFrame with firstâ€‘order Sobol indices."""
    # Problem definition
    problem = {
        "num_vars": 7,
        "names": ["a1", "a2", "a3", "b0", "CR", "PP", "L"],
        "bounds": [
            [0.5, 1.0],
            [0.5, 1.0],
            [0.5, 1.0],
            [0.0, 1.0],
            [0.0, 0.5],
            [0.0, 0.5],
            [0.0, 50.0],
        ],
    }
    # SALib â‰¤1.5: seed å¼•æ•°ã¯éå¯¾å¿œã€‚NumPy ã«ã‚·ãƒ¼ãƒ‰å›ºå®šã§å†ç¾æ€§ã‚’æ‹…ä¿
    np.random.seed(0)
    # ãƒ‘ãƒƒãƒâ‘¡: N ã‚’ 2^n ã«ä¸¸ã‚ã‚‹
    N_pow2 = 2 ** math.ceil(math.log2(N))
    if N_pow2 != N:
        st.info(f"Sobol sampleæ•°ã‚’ {N} â†’ {N_pow2} ã«èª¿æ•´ï¼ˆ2^n å¿…é ˆï¼‰")
    X = sobol_sample(problem, N_pow2, calc_second_order=False)
    # Vectorised model: compute E_total for each row
    qual_T, qual_B = (1, 1) if p["qual"] == "Standard" else (2 / 3, 0.8)
    sched_T, sched_B = (1, 1) if p["sched"] == "OnTime" else (2 / 3, 0.8)

    a_tot = X[:, 0] * X[:, 1] * X[:, 2]
    b_eff = X[:, 3] * qual_B * sched_B
    Sarr = 1 - (1 - a_tot) * (1 - b_eff)

    # Costs
    T_base = (p["T1"] + p["T2"] + p["T3"]) * qual_T * sched_T  # keep times constant
    C_base = T_base * (1 + X[:, 4] + X[:, 5])
    E_total_arr = (C_base + X[:, 6] * C_base * (1 - Sarr)) / Sarr

    Si = sobol.analyze(problem, E_total_arr, calc_second_order=False, print_to_console=False)
    df = pd.DataFrame({"Parameter": problem["names"], "S1": Si["S1"], "ST": Si["ST"]})
    return df.sort_values("S1", ascending=False)


df_sobol = run_sobol(params)

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
#  SectionÂ 8  â€¢  Deterministic baseline computation
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
qual_T, qual_B = (1, 1) if params["qual"] == "Standard" else (2 / 3, 0.8)
sched_T, sched_B = (1, 1) if params["sched"] == "OnTime" else (2 / 3, 0.8)
a_total = params["a1"] * params["a2"] * params["a3"]
b_eff = params["b0"] * qual_B * sched_B
S = 1 - (1 - a_total) * (1 - b_eff)
T = (params["T1"] + params["T2"] + params["T3"]) * qual_T * sched_T
C = T * (1 + params["cross_ratio"] + params["prep_post_ratio"])
C_loss = C + params["loss_unit"] * C * (1 - S)
E = C / S
E_total = C_loss / S

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
#  SectionÂ 9  â€¢  Relative & standardized local elasticities
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def symbolic_derivatives(C_: float, S_: float, L_: float) -> Dict[str, float]:
    C_sym, S_sym, L_sym = sp.symbols("C S L")
    E_expr = (C_sym + C_sym * L_sym * (1 - S_sym)) / S_sym
    return {
        "dE_dC": float(sp.diff(E_expr, C_sym).subs({C_sym: C_, S_sym: S_, L_sym: L_})),
        "dE_dS": float(sp.diff(E_expr, S_sym).subs({C_sym: C_, S_sym: S_, L_sym: L_})),
        "dE_dL": float(sp.diff(E_expr, L_sym).subs({C_sym: C_, S_sym: S_, L_sym: L_})),
    }


derivs = symbolic_derivatives(C, S, params["loss_unit"])
rel_C = derivs["dE_dC"] * C / E_total
rel_S = derivs["dE_dS"] * S / E_total
rel_L = derivs["dE_dL"] * params["loss_unit"] / E_total
std_C = derivs["dE_dC"] * ÏƒC / ÏƒE
std_S = derivs["dE_dS"] * ÏƒS / ÏƒE
std_L = derivs["dE_dL"] * ÏƒL / ÏƒE

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
#  SectionÂ 10  â€¢  UI layout
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
left, right = st.columns([1, 2])
with left:
    st.subheader(TXT["panel"]["output"])
    st.metric(TXT["metrics"]["a_total"], f"{a_total:.4f}")
    st.metric(TXT["metrics"]["succ"], f"{S:.2%}")
    st.metric(TXT["metrics"]["C"], f"{C:.1f}")
    st.metric(TXT["metrics"]["Closs"], f"{C_loss:.1f}")
    st.metric(TXT["metrics"]["E_base"], f"{E:.1f}")
    st.metric(TXT["metrics"]["E_total"], f"{E_total:.1f}")

with right:
    # Quality Ã— Schedule matrix â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    st.subheader(TXT["charts"]["quality_schedule"]["title"])
    exp("charts.quality_schedule.expander_title")

    scenarios = [
        ("Std/On",  "Standard", "OnTime"),
        ("Std/Late","Standard", "Late"),
        ("Low/On",  "Low",      "OnTime"),
        ("Low/Late","Low",      "Late"),
    ]
    bars = []
    for label, qg, scd in scenarios:
        S_qs, _, _, _, E_tot_qs = compute_metrics(
            a1v=params["a1"], a2v=params["a2"], a3v=params["a3"],
            bv=params["b0"],
            cross_ratio_v=params["cross_ratio"],
            prep_post_ratio_v=params["prep_post_ratio"],
            loss_unit_v=params["loss_unit"],
            qualv=qg, schedv=scd,
            t1v=params["T1"], t2v=params["T2"], t3v=params["T3"],
        )
        bars.append(dict(Scenario=label, E_total=E_tot_qs, S=f"{S_qs:.1%}"))

    df_qs = pd.DataFrame(bars)
    fig_qs = px.bar(
        df_qs, x="Scenario", y="E_total", text="S",
        color_discrete_sequence=["#000000"],
        labels={"E_total": TXT["metrics"]["E_total"], "Scenario": ""}
    )
    fig_qs.update_traces(textposition="auto",
                         insidetextfont_color="white",
                         outsidetextfont_color="gray")
    fig_qs.update_layout(font=dict(size=14), bargap=0.1, margin=dict(t=30, b=40))
    st.plotly_chart(fig_qs, use_container_width=True)

    # Tornado local sensitivity  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    st.subheader(TXT["charts"]["tornado"]["title"])
    exp("charts.tornado.expander_title")

    # â‘  å½±éŸ¿åº¦ã‚’å†è¨ˆç®—ï¼ˆÂ±20â€¯%ï¼‰
    sens_targets = {
        "a1": params["a1"],
        "a2": params["a2"],
        "a3": params["a3"],
        "b0": params["b0"],
        "CR": params["cross_ratio"],
        "PP": params["prep_post_ratio"],
        "L":  params["loss_unit"],
    }
    # å¼•æ•°åãƒãƒƒãƒ”ãƒ³ã‚°
    name_map = {
        "a1": "a1v", "a2": "a2v", "a3": "a3v",
        "b0": "bv",  "CR": "cross_ratio_v",
        "PP": "prep_post_ratio_v", "L": "loss_unit_v",
    }
    tornado_rows: List[Tuple[str, float]] = []
    for key, base in sens_targets.items():
        lo = max(base * 0.8, 0)
        hi = base * 1.2 if key not in ("a1", "a2", "a3", "b0") else min(base * 1.2, 1)

        def e_total_at(**override):
            kw = dict(
                a1v=params["a1"], a2v=params["a2"], a3v=params["a3"],
                bv=params["b0"], cross_ratio_v=params["cross_ratio"],
                prep_post_ratio_v=params["prep_post_ratio"], loss_unit_v=params["loss_unit"],
                qualv=params["qual"], schedv=params["sched"],
                t1v=params["T1"], t2v=params["T2"], t3v=params["T3"],
            )
            kw.update(override)
            return compute_metrics(**kw)[4]  # E_total

        delta = max(
            abs(e_total_at(**{name_map[key]: lo}) - E_total),
            abs(e_total_at(**{name_map[key]: hi}) - E_total),
        ) / E_total  # ç›¸å¯¾å¤‰åŒ– (0â€“1)

        tornado_rows.append((key, delta))

    df_tornado = (
        pd.DataFrame(tornado_rows, columns=["Parameter", "RelChange"])
        .sort_values("RelChange", ascending=False)
    )

    fig_tornado = make_sensitivity_bar(
        df_tornado.rename(columns={"RelChange": "Tornado"}),
        value_col="Tornado",
        tick_fmt="{:.2%}",
        order=df_tornado["Parameter"].tolist(),
    )
    st.plotly_chart(fig_tornado, use_container_width=True)

    # ---------------- NEW: Sobol global sensitivity ------------
    st.subheader(TXT["charts"]["sobol"]["title"])
    exp("charts.sobol.expander_title")

    fig_sobol = make_sensitivity_bar(
        df_sobol[["Parameter", "S1"]].rename(columns={"S1": "Sobol"}),
        value_col="Sobol",
        tick_fmt="{:.2f}",
        order=df_sobol["Parameter"].tolist(),
    )
    st.plotly_chart(fig_sobol, use_container_width=True)

    # ----------------------------------------------------------
    # Relative & Standardised sensitivity charts
    # ----------------------------------------------------------
    sens_df = pd.DataFrame(
        dict(
            Parameter=[TXT["metrics"]["loss_unit"],
                       TXT["metrics"]["C"],
                       TXT["metrics"]["succ"]],
            Relative=[abs(rel_L), abs(rel_C), abs(rel_S)],
            Standardised=[abs(std_L), abs(std_C), abs(std_S)],
        )
    )
    order = sens_df["Parameter"].tolist()

    fig_rel = make_sensitivity_bar(
        sens_df[["Parameter", "Relative"]].rename(columns={"Relative": "rel"}),
        value_col="rel",
        order=order,
        tick_fmt="{:.2f}",
    )
    fig_std = make_sensitivity_bar(
        sens_df[["Parameter", "Standardised"]].rename(columns={"Standardised": "std"}),
        value_col="std",
        order=order,
        tick_fmt="{:.3f}",
    )

#  Twoâ€‘column layout for sensitivity plots
col_rel, col_std = st.columns(2)
with col_rel:
    col_rel.subheader(TXT["charts"]["relative_sensitivity"]["title"])
    exp("charts.relative_sensitivity.expander_title")
    col_rel.plotly_chart(fig_rel, use_container_width=True)
with col_std:
    col_std.subheader(TXT["charts"]["standardized_sensitivity"]["title"])
    exp("charts.standardized_sensitivity.expander_title")
    col_std.plotly_chart(fig_std, use_container_width=True)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  Monteâ€‘Carlo histogram
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
st.subheader(TXT["charts"]["monte_carlo"]["title"])
exp("charts.monte_carlo.expander_title")

mc_var = params["mc_var"]
data = Evals if mc_var == "E_total" else Svals
ci_low, ci_high = np.percentile(data, [5, 95])
mean, median = data.mean(), np.median(data)
dec = ".2f" if mc_var == "E_total" else ".4f"

col_m1, col_m2, col_m3 = st.columns(3)
with col_m1:
    st.metric(TXT["charts"]["monte_carlo"]["mean"] + TXT["charts"]["monte_carlo"]["card_unit"],
              f"{mean:{dec}}")
with col_m2:
    st.metric(TXT["charts"]["monte_carlo"]["median"] + TXT["charts"]["monte_carlo"]["card_unit"],
              f"{median:{dec}}")
with col_m3:
    st.metric(TXT["charts"]["monte_carlo"]["ci"] + TXT["charts"]["monte_carlo"]["card_unit"],
              f"{ci_low:{dec}} â€“ {ci_high:{dec}}")

LABEL_E = "E_total" if lang == "English" else "E_total: ç·åˆåŠ¹ç‡"
LABEL_CNT = "Count" if lang == "English" else "é »åº¦"

fig_hist = px.histogram(
    data,
    nbins=100,
    labels={"value": LABEL_E},
    color_discrete_sequence=["#000000"],
)
fig_hist.update_traces(marker_line_width=0.5)
fig_hist.add_vline(x=mean, line_dash="solid", line_color="#000000")
fig_hist.add_vline(x=median, line_dash="solid", line_color="#000000")
fig_hist.add_vline(x=ci_low, line_dash="dot", line_color="#000000")
fig_hist.add_vline(x=ci_high, line_dash="dot", line_color="#000000")
fig_hist.update_layout(
    yaxis_title=LABEL_CNT,
    bargap=0.01,
    margin=dict(t=70),
    showlegend=False,
)
st.plotly_chart(fig_hist, use_container_width=True)

legend = "Mean / Median: solidâ€ƒÂ 5â€“95â€¯% CI: dotted" if lang == "English" \
    else "å¹³å‡ / ä¸­å¤®å€¤ï¼šå®Ÿç·šâ€ƒÂ ä¿¡é ¼åŒºé–“5â€“95â€¯%ï¼šç‚¹ç·š"
st.caption(legend)